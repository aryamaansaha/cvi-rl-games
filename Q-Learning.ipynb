{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untrained Agent without Reinforcement Learning  \n",
    "Here, the agent plays by sampling random actions from it's action space. It obviously does terribly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(500)\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "print(env.observation_space)\n",
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state and action space both are discrete for this game environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 | Score: -355 | Penalties: 31\n",
      "Episode 1 | Score: -704 | Penalties: 56\n",
      "Episode 2 | Score: -704 | Penalties: 56\n",
      "Episode 3 | Score: -785 | Penalties: 65\n",
      "Episode 4 | Score: -866 | Penalties: 74\n",
      "Episode 5 | Score: -758 | Penalties: 62\n",
      "Episode 6 | Score: -545 | Penalties: 45\n",
      "Episode 7 | Score: -758 | Penalties: 62\n",
      "Episode 8 | Score: -794 | Penalties: 66\n",
      "Episode 9 | Score: -713 | Penalties: 57\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    score, penalties = 0, 0\n",
    "    done = False\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        action = env.action_space.sample()\n",
    "        state_, reward, done, _ = env.step(action)\n",
    "        score += reward\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "        state = state_\n",
    "    print(f'Episode {i} | Score: {score} | Penalties: {penalties}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Q-Learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, epsilon, alpha, gamma):\n",
    "        self.env = env\n",
    "        self.Q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        self.epsilon = epsilon\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"state_\"])\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(self.Q_table[state])\n",
    "        return action\n",
    "    \n",
    "    def learn(self, xp):\n",
    "        state, action, reward, state_ = xp\n",
    "        next_qmax = np.max(self.Q_table[state_])\n",
    "        self.Q_table[state, action] = (1 - self.alpha) * self.Q_table[state, action] \\\n",
    "                                                + self.alpha * (reward + self.gamma * next_qmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000 | Score: -11 | Avg Timesteps: 46.5 | Avg Penalties: 1.61\n",
      "Episode 2000 | Score: -67 | Avg Timesteps: 20.72 | Avg Penalties: 0.62\n",
      "Episode 3000 | Score: 2 | Avg Timesteps: 17.04 | Avg Penalties: 0.56\n",
      "Episode 4000 | Score: 11 | Avg Timesteps: 17.49 | Avg Penalties: 0.55\n",
      "Episode 5000 | Score: 11 | Avg Timesteps: 15.51 | Avg Penalties: 0.45\n",
      "Episode 6000 | Score: -7 | Avg Timesteps: 14.74 | Avg Penalties: 0.47\n",
      "Episode 7000 | Score: 7 | Avg Timesteps: 15.75 | Avg Penalties: 0.39\n",
      "Episode 8000 | Score: 8 | Avg Timesteps: 15.3 | Avg Penalties: 0.48\n",
      "Episode 9000 | Score: -4 | Avg Timesteps: 14.64 | Avg Penalties: 0.33\n",
      "Episode 10000 | Score: -1 | Avg Timesteps: 15.27 | Avg Penalties: 0.49\n",
      "Episode 11000 | Score: -10 | Avg Timesteps: 15.37 | Avg Penalties: 0.51\n",
      "Episode 12000 | Score: 11 | Avg Timesteps: 14.68 | Avg Penalties: 0.43\n",
      "Episode 13000 | Score: 5 | Avg Timesteps: 14.86 | Avg Penalties: 0.47\n",
      "Episode 14000 | Score: 4 | Avg Timesteps: 14.69 | Avg Penalties: 0.52\n",
      "Episode 15000 | Score: -2 | Avg Timesteps: 14.63 | Avg Penalties: 0.33\n",
      "Episode 16000 | Score: 7 | Avg Timesteps: 14.99 | Avg Penalties: 0.47\n",
      "Episode 17000 | Score: -13 | Avg Timesteps: 14.35 | Avg Penalties: 0.49\n",
      "Episode 18000 | Score: 12 | Avg Timesteps: 14.7 | Avg Penalties: 0.48\n",
      "Episode 19000 | Score: -1 | Avg Timesteps: 14.31 | Avg Penalties: 0.54\n",
      "Episode 20000 | Score: 11 | Avg Timesteps: 14.45 | Avg Penalties: 0.46\n",
      "Episode 21000 | Score: 9 | Avg Timesteps: 14.74 | Avg Penalties: 0.44\n",
      "Episode 22000 | Score: -13 | Avg Timesteps: 15.28 | Avg Penalties: 0.54\n",
      "Episode 23000 | Score: -2 | Avg Timesteps: 14.92 | Avg Penalties: 0.34\n",
      "Episode 24000 | Score: -17 | Avg Timesteps: 15.15 | Avg Penalties: 0.59\n",
      "Episode 25000 | Score: -6 | Avg Timesteps: 14.35 | Avg Penalties: 0.4\n",
      "Episode 26000 | Score: -2 | Avg Timesteps: 14.83 | Avg Penalties: 0.39\n",
      "Episode 27000 | Score: 13 | Avg Timesteps: 14.32 | Avg Penalties: 0.39\n",
      "Episode 28000 | Score: 11 | Avg Timesteps: 14.5 | Avg Penalties: 0.36\n",
      "Episode 29000 | Score: -14 | Avg Timesteps: 15.18 | Avg Penalties: 0.4\n",
      "Episode 30000 | Score: -3 | Avg Timesteps: 14.38 | Avg Penalties: 0.53\n",
      "Episode 31000 | Score: -5 | Avg Timesteps: 15.28 | Avg Penalties: 0.52\n",
      "Episode 32000 | Score: 11 | Avg Timesteps: 14.8 | Avg Penalties: 0.53\n",
      "Episode 33000 | Score: 6 | Avg Timesteps: 14.23 | Avg Penalties: 0.4\n",
      "Episode 34000 | Score: 10 | Avg Timesteps: 14.9 | Avg Penalties: 0.49\n",
      "Episode 35000 | Score: 8 | Avg Timesteps: 14.87 | Avg Penalties: 0.41\n",
      "Episode 36000 | Score: -17 | Avg Timesteps: 14.63 | Avg Penalties: 0.43\n",
      "Episode 37000 | Score: -7 | Avg Timesteps: 14.71 | Avg Penalties: 0.37\n",
      "Episode 38000 | Score: 9 | Avg Timesteps: 14.37 | Avg Penalties: 0.42\n",
      "Episode 39000 | Score: 7 | Avg Timesteps: 14.93 | Avg Penalties: 0.51\n",
      "Episode 40000 | Score: 4 | Avg Timesteps: 14.89 | Avg Penalties: 0.57\n",
      "Episode 41000 | Score: 4 | Avg Timesteps: 14.49 | Avg Penalties: 0.44\n",
      "Episode 42000 | Score: 13 | Avg Timesteps: 14.7 | Avg Penalties: 0.53\n",
      "Episode 43000 | Score: 3 | Avg Timesteps: 15.08 | Avg Penalties: 0.42\n",
      "Episode 44000 | Score: 3 | Avg Timesteps: 15.21 | Avg Penalties: 0.41\n",
      "Episode 45000 | Score: 2 | Avg Timesteps: 14.92 | Avg Penalties: 0.44\n",
      "Episode 46000 | Score: 5 | Avg Timesteps: 14.71 | Avg Penalties: 0.35\n",
      "Episode 47000 | Score: -2 | Avg Timesteps: 14.57 | Avg Penalties: 0.44\n",
      "Episode 48000 | Score: -4 | Avg Timesteps: 15.06 | Avg Penalties: 0.44\n",
      "Episode 49000 | Score: 8 | Avg Timesteps: 14.63 | Avg Penalties: 0.39\n",
      "Episode 50000 | Score: 7 | Avg Timesteps: 15.0 | Avg Penalties: 0.49\n",
      "CPU times: user 25.1 s, sys: 3.65 s, total: 28.7 s\n",
      "Wall time: 24.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "env = gym.make('Taxi-v3')\n",
    "agent = Agent(env, epsilon=0.1, alpha=0.1, gamma=0.6)\n",
    "scores, timesteps, total_penalties = [], [], []\n",
    "\n",
    "for i in range(50000):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    done = False\n",
    "    t, penalties = 0, 0\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        action = agent.choose_action(state)\n",
    "        state_, reward, done, _ = env.step(action)\n",
    "        score += reward\n",
    "        xp = agent.experience(state, action, reward, state_)\n",
    "        agent.learn(xp)\n",
    "        state = state_\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "        t += 1\n",
    "    timesteps.append(t)\n",
    "    total_penalties.append(penalties)\n",
    "    if (i+1) % 1000 == 0:\n",
    "        print(f'Episode {i+1} | Score: {score} | Avg Timesteps: {np.mean(timesteps[-100:])}',\n",
    "              f'| Avg Penalties: {np.mean(total_penalties[-100:])}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Timesteps: 12.98 | Avg Penalties: 0.0\n"
     ]
    }
   ],
   "source": [
    "total_penalties, scores, timesteps = [], [], []\n",
    "for i in range(100):\n",
    "    state = env.reset()\n",
    "    score, penalties, t = 0, 0, 0\n",
    "    done = False\n",
    "    while not done:\n",
    "#         env.render()\n",
    "        action = np.argmax(agent.Q_table[state])\n",
    "        state_, reward, done, _ = env.step(action)\n",
    "        score += reward\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "        state = state_\n",
    "        t += 1\n",
    "    scores.append(score)\n",
    "    total_penalties.append(penalties)\n",
    "    timesteps.append(t)\n",
    "print(f'Avg Timesteps: {np.mean(timesteps)} | Avg Penalties: {np.mean(total_penalties)}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-learning model performs really well - now the agent incurs 0 penalties and takes minimum steps to drop the passenger off to the right location!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
